{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42168abd-f5e8-4bd9-8ef7-459d48bfd910",
      "metadata": {
        "id": "42168abd-f5e8-4bd9-8ef7-459d48bfd910"
      },
      "outputs": [],
      "source": [
        "###THEANO_FLAGS=mode=FAST_RUN,device=gpu0,floatX=float32 python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras import regularizers\n",
        "from numpy import interp\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def prepare_data(separate=False):\n",
        "    \"\"\"\n",
        "    Load and prepare piRNA-disease association data.\n",
        "    Returns concatenated feature vectors and labels.\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # Load similarity matrices and association matrix\n",
        "    disease_sim = pd.read_csv(\"/content/drive/My Drive/Data/d2d_do.csv\",\n",
        "                              header=0, index_col=0).to_numpy()\n",
        "    piRNA_sim = pd.read_csv(\"/content/drive/My Drive/Data/p2p_smith.csv\",\n",
        "                            header=0, index_col=0).to_numpy()\n",
        "    associations = pd.read_csv(\"/content/drive/My Drive/Data/adj.csv\",\n",
        "                               header=0, index_col=0).to_numpy().T\n",
        "\n",
        "    # Extract relevant portions\n",
        "    piRNA_sim = piRNA_sim[:2174, :2174]\n",
        "    associations = associations[:, :2174]\n",
        "\n",
        "    print(f\"Association matrix shape: {associations.shape}\")\n",
        "    print(f\"piRNA similarity matrix shape: {piRNA_sim.shape}\")\n",
        "    print(f\"Disease similarity matrix shape: {disease_sim.shape}\")\n",
        "\n",
        "    n_diseases = associations.shape[0]\n",
        "    n_piRNAs = associations.shape[1]\n",
        "\n",
        "    positive_samples = []\n",
        "    negative_samples = []\n",
        "    positive_labels = []\n",
        "    negative_labels = []\n",
        "\n",
        "    # Generate feature vectors for each piRNA-disease pair\n",
        "    for i in range(n_diseases):\n",
        "        for j in range(n_piRNAs):\n",
        "            # piRNA feature: similarity scores with all other piRNAs\n",
        "            piRNA_features = list(piRNA_sim[j, :])\n",
        "\n",
        "            # Disease feature: similarity scores with all other diseases\n",
        "            disease_features = list(disease_sim[i, :])\n",
        "\n",
        "            # Concatenate to create feature vector of size (n_piRNAs + n_diseases)\n",
        "            feature_vector = piRNA_features + disease_features\n",
        "\n",
        "            if associations[i, j] == 1:\n",
        "                # Known association (positive sample)\n",
        "                positive_samples.append(feature_vector)\n",
        "                positive_labels.append(1)\n",
        "            else:\n",
        "                # Unknown association (potential negative sample)\n",
        "                negative_samples.append(feature_vector)\n",
        "                negative_labels.append(0)\n",
        "\n",
        "    print(f\"Number of positive samples: {len(positive_samples)}\")\n",
        "    print(f\"Number of negative samples: {len(negative_samples)}\")\n",
        "\n",
        "    # Randomly sample negative samples equal to positive samples\n",
        "    n_positive = len(positive_samples)\n",
        "    negative_indices = np.random.permutation(len(negative_samples))[:n_positive]\n",
        "\n",
        "    selected_negative_samples = [negative_samples[i] for i in negative_indices]\n",
        "    selected_negative_labels = [negative_labels[i] for i in negative_indices]\n",
        "\n",
        "    # Combine positive and negative samples\n",
        "    all_samples = positive_samples + selected_negative_samples\n",
        "    all_labels = positive_labels + selected_negative_labels\n",
        "\n",
        "    print(f\"Total balanced samples: {len(all_samples)}\")\n",
        "    print(f\"Feature vector size: {len(all_samples[0])}\")\n",
        "\n",
        "    return np.array(all_samples), np.array(all_labels)\n",
        "\n",
        "\n",
        "def build_sparse_autoencoder(input_dim, encoding_dim=128):\n",
        "    \"\"\"\n",
        "    Build a sparse autoencoder for feature extraction.\n",
        "\n",
        "    Args:\n",
        "        input_dim: Dimension of input features\n",
        "        encoding_dim: Dimension of encoded representation\n",
        "\n",
        "    Returns:\n",
        "        autoencoder: Full autoencoder model\n",
        "        encoder: Encoder part only\n",
        "    \"\"\"\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(350, activation='relu')(input_layer)\n",
        "    encoded = Dense(250, activation='relu')(encoded)\n",
        "    encoded = Dense(100, activation='relu')(encoded)\n",
        "\n",
        "    # Latent representation with L1 regularization for sparsity\n",
        "    latent = Dense(encoding_dim, activation='relu',\n",
        "                   activity_regularizer=regularizers.l1(1e-5))(encoded)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = Dense(100, activation='relu')(latent)\n",
        "    decoded = Dense(250, activation='relu')(decoded)\n",
        "    decoded = Dense(350, activation='relu')(decoded)\n",
        "    output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "    # Models\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    encoder = Model(inputs=input_layer, outputs=latent)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "\n",
        "def calculate_performance(y_true, y_pred):\n",
        "    \"\"\"Calculate performance metrics.\"\"\"\n",
        "    tp = fp = tn = fn = 0\n",
        "\n",
        "    for true_label, pred_label in zip(y_true, y_pred):\n",
        "        if true_label == 1:\n",
        "            if pred_label == 1:\n",
        "                tp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "        else:\n",
        "            if pred_label == 0:\n",
        "                tn += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "\n",
        "    test_num = len(y_true)\n",
        "    acc = (tp + tn) / test_num\n",
        "\n",
        "    if tp == 0 and fp == 0:\n",
        "        precision = 0\n",
        "        mcc = 0\n",
        "        f1_score = 0\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    else:\n",
        "        precision = tp / (tp + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "        specificity = tn / (tn + fp)\n",
        "\n",
        "        denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "        mcc = (tp * tn - fp * fn) / denominator if denominator > 0 else 0\n",
        "\n",
        "        f1_score = (2 * tp) / (2 * tp + fp + fn)\n",
        "\n",
        "    return acc, precision, sensitivity, specificity, mcc, f1_score\n",
        "\n",
        "\n",
        "def piR_LGBM():\n",
        "    \"\"\"\n",
        "    Main function implementing the piR-LGBM model for piRNA-disease association prediction.\n",
        "    \"\"\"\n",
        "    # Load and prepare data\n",
        "    X, y = prepare_data()\n",
        "\n",
        "    # Shuffle data\n",
        "    indices = np.arange(len(y))\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Cross-validation setup\n",
        "    num_folds = 5\n",
        "    fold_size = len(y) // num_folds\n",
        "\n",
        "    # Storage for results\n",
        "    all_performance = []\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Starting 5-Fold Cross-Validation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for fold in range(num_folds):\n",
        "        print(f\"\\n--- Fold {fold + 1}/{num_folds} ---\")\n",
        "\n",
        "        # Split data for this fold\n",
        "        test_indices = list(range(fold * fold_size, (fold + 1) * fold_size))\n",
        "        train_indices = list(set(range(len(y))) - set(test_indices))\n",
        "\n",
        "        X_train = X[train_indices]\n",
        "        X_test = X[test_indices]\n",
        "        y_train = y[train_indices]\n",
        "        y_test = y[test_indices]\n",
        "\n",
        "        print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "\n",
        "        # Step 1: Train Sparse Autoencoder\n",
        "        print(\"Training Sparse Autoencoder...\")\n",
        "        input_dim = X_train.shape[1]\n",
        "        autoencoder, encoder = build_sparse_autoencoder(input_dim, encoding_dim=128)\n",
        "\n",
        "        autoencoder.fit(X_train, X_train,\n",
        "                       epochs=20,\n",
        "                       batch_size=100,\n",
        "                       shuffle=True,\n",
        "                       verbose=0)\n",
        "\n",
        "        # Step 2: Extract encoded features\n",
        "        print(\"Extracting encoded features...\")\n",
        "        X_train_encoded = encoder.predict(X_train, verbose=0)\n",
        "        X_test_encoded = encoder.predict(X_test, verbose=0)\n",
        "\n",
        "        print(f\"Encoded feature dimension: {X_train_encoded.shape[1]}\")\n",
        "\n",
        "        # Step 3: Train LightGBM Classifier\n",
        "        print(\"Training LightGBM Classifier...\")\n",
        "        params = {\n",
        "            'objective': 'binary',\n",
        "            'metric': 'binary_logloss',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.05,\n",
        "            'feature_fraction': 0.9,\n",
        "            'verbose': -1\n",
        "        }\n",
        "\n",
        "        train_data = lgb.Dataset(X_train_encoded, label=y_train)\n",
        "        test_data = lgb.Dataset(X_test_encoded, label=y_test, reference=train_data)\n",
        "\n",
        "        bst = lgb.train(params, train_data, num_boost_round=100,\n",
        "                       valid_sets=[test_data])\n",
        "\n",
        "        # Step 4: Predict and evaluate\n",
        "        y_pred_prob = bst.predict(X_test_encoded)\n",
        "        y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        acc, precision, sensitivity, specificity, mcc, f1 = calculate_performance(y_test, y_pred)\n",
        "\n",
        "        # ROC curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Precision-Recall curve\n",
        "        prec_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "        aupr = auc(recall_vals, prec_vals)\n",
        "\n",
        "        print(f\"Accuracy: {acc:.4f}, Precision: {precision:.4f}, Sensitivity: {sensitivity:.4f}\")\n",
        "        print(f\"Specificity: {specificity:.4f}, MCC: {mcc:.4f}, AUC: {roc_auc:.4f}, AUPR: {aupr:.4f}\")\n",
        "\n",
        "        all_performance.append([acc, precision, sensitivity, specificity, mcc, roc_auc, aupr, f1])\n",
        "\n",
        "        # Plot ROC curve for this fold\n",
        "        plt.plot(fpr, tpr, alpha=0.5,\n",
        "                label=f'Fold {fold + 1} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "        # Interpolate for mean ROC curve\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "        mean_tpr[0] = 0.0\n",
        "\n",
        "    # Calculate mean performance\n",
        "    mean_tpr /= num_folds\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    mean_performance = np.mean(all_performance, axis=0)\n",
        "    std_performance = np.std(all_performance, axis=0)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"piR-LGBM Results (5-Fold Cross-Validation)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Accuracy:     {mean_performance[0]:.4f} ± {std_performance[0]:.4f}\")\n",
        "    print(f\"Precision:    {mean_performance[1]:.4f} ± {std_performance[1]:.4f}\")\n",
        "    print(f\"Sensitivity:  {mean_performance[2]:.4f} ± {std_performance[2]:.4f}\")\n",
        "    print(f\"Specificity:  {mean_performance[3]:.4f} ± {std_performance[3]:.4f}\")\n",
        "    print(f\"MCC:          {mean_performance[4]:.4f} ± {std_performance[4]:.4f}\")\n",
        "    print(f\"AUC:          {mean_performance[5]:.4f} ± {std_performance[5]:.4f}\")\n",
        "    print(f\"AUPR:         {mean_performance[6]:.4f} ± {std_performance[6]:.4f}\")\n",
        "    print(f\"F1-Score:     {mean_performance[7]:.4f} ± {std_performance[7]:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Plot mean ROC curve\n",
        "    plt.plot(mean_fpr, mean_tpr, 'b--', linewidth=2.5,\n",
        "            label=f'Mean ROC (AUC = {mean_performance[5]:.4f})')\n",
        "\n",
        "    plt.xlabel('False Positive Rate (1-Specificity)')\n",
        "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "    plt.title('ROC Curve: piR-LGBM (5-Fold Cross-Validation)')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save results\n",
        "    scipy.io.savemat('piR_LGBM_results.mat', {\n",
        "        'mean_fpr': mean_fpr,\n",
        "        'mean_tpr': mean_tpr,\n",
        "        'mean_auc': mean_performance[5],\n",
        "        'all_performance': all_performance\n",
        "    })\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    piR_LGBM()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
